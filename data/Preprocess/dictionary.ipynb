{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import dataframes drom preprocessed news\n",
    "df_1 = pd.read_csv('preprocessed_news/Crypto_Currency_News_f.csv')\n",
    "df_2 = pd.read_csv('preprocessed_news/CryptoCurrencies_f.csv')\n",
    "df_3 = pd.read_csv('preprocessed_news/CryptoCurrency_f.csv')\n",
    "df_4 = pd.read_csv('preprocessed_news/Cryptomarkets_f.csv')\n",
    "df_5 = pd.read_csv('preprocessed_news/eth_f.csv')\n",
    "df_6 = pd.read_csv('preprocessed_news/ethfinance_f.csv')\n",
    "df_7 = pd.read_csv('preprocessed_news/ethtrader_f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lcano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lcano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lcano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# We are going to create a dictionary with all the words in the news\n",
    "# We will use this dictionary to tokenize the news\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# set of stopwords\n",
    "# keep negation words (for all apostrophe negations, the token will become n't)\n",
    "keep = {'no', 'not', 'nor', 'don', 'ain', 'aren', 'couldn', 'didn', 'doesn', \n",
    "        'hadn', 'hasn', 'haven', 'isn', 'mightn', 'mustn', 'needn', 'shan', \n",
    "        'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'more', 'most', 'less'}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "extra_sw = {'http', '\\'s'}\n",
    "stop_words = stop_words.union(extra_sw)\n",
    "\n",
    "# set of punctuation\n",
    "punct = set(string.punctuation) - {'$', '\\''}\n",
    "punct.add('’')\n",
    "# extra_p = {'’', '..', '\\'\\'', '``', '...', '....', \"''\", '...'}\n",
    "# punct = punct.union(extra_p)\n",
    "\n",
    "# lemmatizer, to get the root of the word\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# set of numbers\n",
    "numbers = set('0123456789')\n",
    "\n",
    "# Some important words to add to the dictionary\n",
    "important_words = ['sos', 'eos', 'ethereum', 'eth', 'bitcoin', 'btc', 'dollar', 'usd', 'usdt', 'bull', 'bear', 'dip'\n",
    "                'crypto', 'cryptocurrency', 'buy', 'crash', 'hold', 'moon', 'rocket', 'market', 'rise', 'dump', \n",
    "                'pump', 'binance', 'price', 'volume', 'bnb', 'xrp', 'blockchain', 'altcoin', 'coin', 'high', 'low',\n",
    "                'cap', 'block', 'cryptography', 'decentralized', 'evm', 'gas', 'halve', 'halving', 'hash', 'liquidity',\n",
    "                'mining', 'nft', 'stake', 'supply', 'transaction', 'wallet', 'win', 'loss', 'profit', 'proof-of-stake',\n",
    "                'proof-of-work', 'smart-contract', 'token', 'volatility', 'whale', 'analysis', 'all-time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49783\n",
      "Biggest tokens:\n",
      "[('internationallie3618', 49783), ('kameez', 49782), ('salwar', 49781), ('à¥ˆà¤¶à¤¨', 49780), ('à¤', 49779), ('à¤•à¤¾', 49778), ('qvc', 49777), ('cardino', 49776), ('starfish', 49775), ('safedao', 49774), ('beekeyyy', 49773), ('impunity', 49772), ('dismantle', 49771), ('höptner', 49770), ('tenthlysnog', 49769)]\n",
      "First 100 words in the dictionary\n",
      "[('sos', 1), ('eos', 2), ('ethereum', 3), ('eth', 4), ('bitcoin', 5), ('btc', 6), ('dollar', 7), ('usd', 8), ('usdt', 9), ('bull', 10), ('bear', 11), ('dipcrypto', 12), ('cryptocurrency', 13), ('buy', 14), ('crash', 15), ('hold', 16), ('moon', 17), ('rocket', 18), ('market', 19), ('rise', 20), ('dump', 21), ('pump', 22), ('binance', 23), ('price', 24), ('volume', 25), ('bnb', 26), ('xrp', 27), ('blockchain', 28), ('altcoin', 29), ('coin', 30), ('high', 31), ('low', 32), ('cap', 33), ('block', 34), ('cryptography', 35), ('decentralized', 36), ('evm', 37), ('gas', 38), ('halve', 39), ('halving', 40), ('hash', 41), ('liquidity', 42), ('mining', 43), ('nft', 44), ('stake', 45), ('supply', 46), ('transaction', 47), ('wallet', 48), ('win', 49), ('loss', 50), ('profit', 51), ('proof-of-stake', 52), ('proof-of-work', 53), ('smart-contract', 54), ('token', 55), ('volatility', 56), ('whale', 57), ('analysis', 58), ('all-time', 59), ('divi', 60), ('project', 61), ('update', 62), ('september', 63), ('month', 64), ('review', 65), ('great', 66), ('$', 67), ('guy', 68), ('keeping', 69), ('busy', 70), ('wait', 71), ('mobile', 72), ('heard', 73), ('going', 74), ('demo', 75), ('vega', 76), ('week', 77), ('wish', 78), ('could', 79), ('go', 80), ('want', 81), ('world', 82), ('use', 83), ('crypto', 84), ('technology', 85), ('need', 86), ('easy', 87), ('thank', 88), ('god', 89), ('team', 90), ('got', 91), ('mind', 92), ('interview', 93), ('ceo', 94), ('geoff', 95), ('mccabe', 96), ('podcast', 97), ('unbelievable', 98), ('true', 99), ('expect', 100)]\n"
     ]
    }
   ],
   "source": [
    "# create dictionary\n",
    "def create_dictionary(df_1, df_2, df_3, df_4, df_5, df_6, df_7):\n",
    "    # dictionary = {word: token}\n",
    "    dictionary = defaultdict(int)\n",
    "    token = 1\n",
    "    for word in important_words:\n",
    "        dictionary[word] = token\n",
    "        token += 1\n",
    "    # iterate over the news\n",
    "    # df.values are text_1 to text_10 of each day\n",
    "    for df in [df_1, df_2, df_3, df_4, df_5, df_6, df_7]:\n",
    "        for row in df.values:\n",
    "            for text in row:\n",
    "                if type(text) == str:\n",
    "                    # tokenize\n",
    "                    tokens = word_tokenize(text)\n",
    "                    for word in tokens:\n",
    "                        word = word.lower()\n",
    "                        # if word starts with ' remove it (to prevent cases example: why and 'why)\n",
    "                        if word[0] == '\\'':\n",
    "                            if len(word) == 1:\n",
    "                                continue\n",
    "                            word = word[1:]\n",
    "                        # remove punctuation at the end of the word\n",
    "                        if word[-1] in punct:\n",
    "                            if len(word) == 1:\n",
    "                                continue\n",
    "                            word = word[:-1]\n",
    "                        # many words with punctuation in the middle such as utc-12:00-somtehing, 200k-300, https://www.something.com ...\n",
    "                        # as we have a very large dictionary and these words have little meaning, we will remove them\n",
    "                        if not any((c in punct) for c in word):\n",
    "                            # remove numbers\n",
    "                            if word[0] in numbers:\n",
    "                                continue\n",
    "                            # remove stopwords\n",
    "                            if word in stop_words:\n",
    "                                continue\n",
    "                            # lemmatize\n",
    "                            # do not lemmatize SOS and EOS\n",
    "                            if word != 'sos' and word != 'eos':\n",
    "                                word = lemmatizer.lemmatize(word)\n",
    "                                # is possible that the word root is a stopword\n",
    "                                if word in stop_words:\n",
    "                                    continue\n",
    "                            # add to dictionary\n",
    "                            if  dictionary[word] == 0:\n",
    "                                # dictionary[word] = len(dictionary) + 1\n",
    "                                dictionary[word] = token\n",
    "                                token += 1\n",
    "    return dictionary\n",
    "\n",
    "    \n",
    "# create dictionary\n",
    "dictionary = create_dictionary(df_1, df_2, df_3, df_4, df_5, df_6, df_7)\n",
    "\n",
    "# print size of dictionary\n",
    "print(len(dictionary))\n",
    "# print top 15 most frequent words\n",
    "print(\"Biggest tokens:\")\n",
    "print(sorted(dictionary.items(), key=lambda x: x[1], reverse=True)[:15])\n",
    "\n",
    "# print the dictionary first 100 words\n",
    "print(\"First 100 words in the dictionary\")\n",
    "print(list(dictionary.items())[:100])\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency dictionary\n",
    "# def create_dictionary_freq(df):\n",
    "    # dictionary = defaultdict(int)\n",
    "    # # Some important words to add to the dictionary\n",
    "    # important_words = ['sos', 'eos', 'ethereum', 'bitcoin', 'eth', 'btc']\n",
    "    # for word in important_words:\n",
    "    #     dictionary[word] = 1\n",
    "    # for news in df.values: # df.values are text_1 to text_10 of each day\n",
    "    #     for text in news:\n",
    "    #         if type(text) == str:\n",
    "    #             for word in word_tokenize(text):\n",
    "    #                 word = word.lower()\n",
    "    #                 # if word starts with ' remove it (to prevent cases example: why and 'why)\n",
    "    #                 # but skip to next word if the word is only '\n",
    "    #                 if word[0] == '\\'':\n",
    "    #                     if len(word) == 1:\n",
    "    #                         continue\n",
    "    #                     word = word[1:]\n",
    "    #                 # check that the word does not have any char from special \n",
    "    #                 if not any((c in punct) for c in word):\n",
    "    #                     if word not in stop_words and word not in numbers:\n",
    "    #                         # do not lemmatize SOS and EOS\n",
    "    #                         if word != 'sos' and word != 'eos':\n",
    "    #                             word = lemmatizer.lemmatize(word)\n",
    "    #                             # is possible that the word root is a stopword\n",
    "    #                             if word in stop_words:\n",
    "    #                                 continue\n",
    "    #                         dictionary[word] += 1\n",
    "    # return dictionary\n",
    "\n",
    "    # dictionary = create_dictionary_freq(df_1)\n",
    "# # add the rest of df to the dictionary, new words will be added, repeated words will be updated by summing the frequency\n",
    "# for word, freq in create_dictionary_freq(df_2).items():\n",
    "#     dictionary[word] += freq\n",
    "# for word, freq in create_dictionary_freq(df_3).items():\n",
    "#     dictionary[word] += freq\n",
    "# for word, freq in create_dictionary_freq(df_4).items():\n",
    "#     dictionary[word] += freq\n",
    "# for word, freq in create_dictionary_freq(df_5).items():\n",
    "#     dictionary[word] += freq\n",
    "# for word, freq in create_dictionary_freq(df_6).items():\n",
    "#     dictionary[word] += freq\n",
    "# for word, freq in create_dictionary_freq(df_7).items():\n",
    "#     dictionary[word] += freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30079\n"
     ]
    }
   ],
   "source": [
    "# access a word token\n",
    "print(dictionary['wreaks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wreaks\n"
     ]
    }
   ],
   "source": [
    "# print word with token 30079\n",
    "for word, token in dictionary.items():\n",
    "    if token == 30079:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "import pickle\n",
    "with open('dictionary.pickle', 'wb') as handle:\n",
    "    pickle.dump(dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load dictionary\n",
    "# with open('dictionary.pickle', 'rb') as handle:\n",
    "#     dictionary = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71124d1c67a53e10234d0517a0d95499fe806d0feef4c01c9260b743b98b34cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
