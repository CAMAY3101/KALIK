{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import dataframes drom preprocessed news\n",
    "df_1 = pd.read_csv('preprocessed_news/Crypto_Currency_News_f.csv')\n",
    "df_2 = pd.read_csv('preprocessed_news/CryptoCurrencies_f.csv')\n",
    "df_3 = pd.read_csv('preprocessed_news/CryptoCurrency_f.csv')\n",
    "df_4 = pd.read_csv('preprocessed_news/Cryptomarkets_f.csv')\n",
    "df_5 = pd.read_csv('preprocessed_news/eth_f.csv')\n",
    "df_6 = pd.read_csv('preprocessed_news/ethfinance_f.csv')\n",
    "df_7 = pd.read_csv('preprocessed_news/ethtrader_f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lcano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lcano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lcano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# We are going to create a dictionary with all the words in the news\n",
    "# We will use this dictionary to tokenize the news\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# set of stopwords\n",
    "# keep negation words (for all apostrophe negations, the token will become n't)\n",
    "keep = {'no', 'not', 'nor', 'don', 'ain', 'aren', 'couldn', 'didn', 'doesn', \n",
    "        'hadn', 'hasn', 'haven', 'isn', 'mightn', 'mustn', 'needn', 'shan', \n",
    "        'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'more', 'most', 'less'}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "extra_sw = {'http', '\\'s'}\n",
    "stop_words = stop_words.union(extra_sw)\n",
    "\n",
    "# set of punctuation\n",
    "punct = set(string.punctuation) - {'$', '\\''}\n",
    "punct.add('â€™')\n",
    "# extra_p = {'â€™', '..', '\\'\\'', '``', '...', '....', \"''\", '...'}\n",
    "# punct = punct.union(extra_p)\n",
    "\n",
    "# lemmatizer, to get the root of the word\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# set of numbers\n",
    "numbers = set('0123456789')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49616\n",
      "Top 15 most frequent words\n",
      "[('marco', 20923), ('renames', 20922), ('vscode', 20921), ('sylva', 20920), ('materially', 20919), ('concealing', 20918), ('reasonsâ€”from', 20917), ('discusion', 20916), ('annihilated', 20915), ('zetachain', 20913), ('unplug', 20912), ('ðŸš€ðŸŽ±', 20910), ('stunner', 20908), ('grating', 20907), ('antic', 20903)]\n",
      "First 100 words in the dictionary\n",
      "[('sos', 1), ('eos', 2), ('ethereum', 3), ('bitcoin', 4), ('eth', 5), ('btc', 6), ('divi', 16109), ('project', 381), ('update', 521), ('september', 141), ('month', 43), ('review', 3723), ('great', 518), ('$', 7), ('guy', 315), ('keeping', 1125), ('busy', 11372), ('wait', 1161), ('mobile', 146), ('wallet', 231), ('heard', 990), ('going', 116), ('demo', 2474), ('vega', 14709), ('blockchain', 161), ('week', 254), ('wish', 1182), ('could', 480), ('go', 106), ('want', 249), ('world', 139), ('use', 220), ('crypto', 156), ('technology', 92), ('need', 344), ('easy', 1979), ('thank', 1307), ('god', 320), ('team', 159), ('got', 15), ('mind', 566), ('interview', 568), ('ceo', 544), ('geoff', 44), ('mccabe', 45), ('podcast', 1421), ('unbelievable', 4885), ('true', 61), ('expect', 591), ('cryptos', 2496), ('ta', 336), ('easier', 458), ('option', 1126), ('market', 263), ('people', 48), ('sei', 56), ('grande', 7173), ('geoffreymccabe', 58), ('bravo', 3564), ('wall', 767), ('street', 230), ('finally', 1832), ('learning', 1991), ('panic', 4853), ('volatility', 2650), ('recent', 1268), ('price', 24), ('dip', 4618), ('bloomberg', 292), ('saying', 1940), ('buy', 490), (\"'\", 224), ('china', 1010), ('large', 1344), ('mining', 3338), ('farm', 7728), ('burned', 2199), ('damage', 4593), ('exceeded', 6148), ('million', 154), ('bull', 685), ('run', 85), ('likely', 1695), ('target', 2355), ('claim', 2586), ('popular', 3050), ('analyst', 306), ('oh', 1629), ('shut', 4276), ('meh', 7666), ('vaultoro', 91), ('integrates', 2300), ('dash', 4988), ('relaunches', 14053), ('trading', 187), ('platform', 164), ('flixxo', 97), ('latest', 380), ('article', 660), ('latam', 100)]\n"
     ]
    }
   ],
   "source": [
    "# create dictionary\n",
    "def create_dictionary(df):\n",
    "    # dictionary = {word: token}\n",
    "    dictionary = defaultdict(int)\n",
    "    token = 1\n",
    "    # Some important words to add to the dictionary\n",
    "    important_words = ['sos', 'eos', 'ethereum', 'bitcoin', 'eth', 'btc']\n",
    "    for word in important_words:\n",
    "        dictionary[word] = token\n",
    "        token += 1\n",
    "    # iterate over the news\n",
    "    # df.values are text_1 to text_10 of each day\n",
    "    for news in df.values:\n",
    "        for text in news:\n",
    "            # tokenize the text\n",
    "            tokens = word_tokenize(text)\n",
    "            for word in tokens:\n",
    "                word = word.lower()\n",
    "                # if word starts with ' remove it (to prevent cases example: why and 'why)\n",
    "                if word[0] == '\\'':\n",
    "                    if len(word) == 1:\n",
    "                        continue\n",
    "                    word = word[1:]\n",
    "                # remove punctuation\n",
    "                if not any((c in punct) for c in word):\n",
    "                    # remove numbers\n",
    "                    if word[0] in numbers:\n",
    "                        continue\n",
    "                    # remove stopwords\n",
    "                    if word in stop_words:\n",
    "                        continue\n",
    "                    # lemmatize\n",
    "                    # do not lemmatize SOS and EOS\n",
    "                    if word != 'sos' and word != 'eos':\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "                        # is possible that the word root is a stopword\n",
    "                        if word in stop_words:\n",
    "                            continue\n",
    "                    # add to dictionary\n",
    "                    if dictionary[word] == 0:\n",
    "                        dictionary[word] = token\n",
    "                        token += 1\n",
    "    return dictionary\n",
    "\n",
    "    # dictionary = defaultdict(int)\n",
    "    # # Some important words to add to the dictionary\n",
    "    # important_words = ['sos', 'eos', 'ethereum', 'bitcoin', 'eth', 'btc']\n",
    "    # for word in important_words:\n",
    "    #     dictionary[word] = 1\n",
    "    # for news in df.values: # df.values are text_1 to text_10 of each day\n",
    "    #     for text in news:\n",
    "    #         if type(text) == str:\n",
    "    #             for word in word_tokenize(text):\n",
    "    #                 word = word.lower()\n",
    "    #                 # if word starts with ' remove it (to prevent cases example: why and 'why)\n",
    "    #                 # but skip to next word if the word is only '\n",
    "    #                 if word[0] == '\\'':\n",
    "    #                     if len(word) == 1:\n",
    "    #                         continue\n",
    "    #                     word = word[1:]\n",
    "    #                 # check that the word does not have any char from special \n",
    "    #                 if not any((c in punct) for c in word):\n",
    "    #                     if word not in stop_words and word not in numbers:\n",
    "    #                         # do not lemmatize SOS and EOS\n",
    "    #                         if word != 'sos' and word != 'eos':\n",
    "    #                             word = lemmatizer.lemmatize(word)\n",
    "    #                             # is possible that the word root is a stopword\n",
    "    #                             if word in stop_words:\n",
    "    #                                 continue\n",
    "    #                         dictionary[word] += 1\n",
    "    # return dictionary\n",
    "\n",
    "# create dictionary\n",
    "dictionary_1 = create_dictionary(df_1)\n",
    "# add the rest of df to the dictionary, new words will be added, repeated words will stay the same\n",
    "dictionary_2 = create_dictionary(df_2)\n",
    "dictionary_3 = create_dictionary(df_3)\n",
    "dictionary_4 = create_dictionary(df_4)\n",
    "dictionary_5 = create_dictionary(df_5)\n",
    "dictionary_6 = create_dictionary(df_6)\n",
    "dictionary_7 = create_dictionary(df_7)\n",
    "\n",
    "# merge dictionaries\n",
    "dictionary = dictionary_1.copy()\n",
    "dictionary.update(dictionary_2)\n",
    "dictionary.update(dictionary_3)\n",
    "dictionary.update(dictionary_4)\n",
    "dictionary.update(dictionary_5)\n",
    "dictionary.update(dictionary_6)\n",
    "dictionary.update(dictionary_7)\n",
    "\n",
    "\n",
    "# dictionary = create_dictionary(df_1)\n",
    "# # add the rest of df to the dictionary, new words will be added, repeated words will be updated by summing the frequency\n",
    "# for word, freq in create_dictionary(df_2).items():\n",
    "#     dictionary[word] += freq\n",
    "# for word, freq in create_dictionary(df_3).items():\n",
    "#     dictionary[word] += freq\n",
    "# for word, freq in create_dictionary(df_4).items():\n",
    "#     dictionary[word] += freq\n",
    "# for word, freq in create_dictionary(df_5).items():\n",
    "#     dictionary[word] += freq\n",
    "# for word, freq in create_dictionary(df_6).items():\n",
    "#     dictionary[word] += freq\n",
    "# for word, freq in create_dictionary(df_7).items():\n",
    "#     dictionary[word] += freq\n",
    "\n",
    "# print size of dictionary\n",
    "print(len(dictionary))\n",
    "# print top 15 most frequent words\n",
    "print(\"Biggest tokens:\")\n",
    "print(sorted(dictionary.items(), key=lambda x: x[1], reverse=True)[:15])\n",
    "\n",
    "# print the dictionary first 100 words\n",
    "print(\"First 100 words in the dictionary\")\n",
    "print(list(dictionary.items())[:100])\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5718\n"
     ]
    }
   ],
   "source": [
    "# access a word token\n",
    "print(dictionary['wreaks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigh\n",
      "wreaks\n",
      "metatransaction\n"
     ]
    }
   ],
   "source": [
    "# print word with token 2295\n",
    "for word, token in dictionary.items():\n",
    "    if token == 5718:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "import pickle\n",
    "with open('dictionary.pickle', 'wb') as handle:\n",
    "    pickle.dump(dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load dictionary\n",
    "# with open('dictionary.pickle', 'rb') as handle:\n",
    "#     dictionary = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71124d1c67a53e10234d0517a0d95499fe806d0feef4c01c9260b743b98b34cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
