{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felip\\Miniconda3\\envs\\MLearn\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentientAnalizer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, vocab_size, embedding_dim):\n",
    "        super(SentientAnalizer, self).__init__()\n",
    "        \"\"\"\n",
    "        [Vocab_size -> Embedding_size]\n",
    "        input_size: [Seq, Batch, Inputs]\n",
    "        \"\"\"\n",
    "        # input_size == 1\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.Vocab_Embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.LSTM = nn.LSTM(self.embedding_dim, self.hidden_size,num_layers=2, batch_first=True)\n",
    "        self.Linear1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.Linear2 = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self,x, hidden):\n",
    "        emb = self.Vocab_Embedding(x).view(x.shape[0],x.shape[1],-1)\n",
    "        out, hidden = self.LSTM(emb, hidden)\n",
    "        out = self.Linear1(out)\n",
    "        out = self.Linear2(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(2, batch_size, self.hidden_size),\n",
    "                torch.zeros(2, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size_stock,input_size_sentiment, hidden_size, num_layers, max_length):\n",
    "        super(StockPredictor, self).__init__()\n",
    "        \"\"\"\n",
    "        [Vocab_size -> Embedding_size]\n",
    "        input_size: [Seq, Batch, Inputs]\n",
    "        \"\"\"\n",
    "        # input_size == 1\n",
    "        self.input_size_stock = input_size_stock\n",
    "        self.input_size_sentiment = input_size_sentiment\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attn_hidd = nn.Linear(self.hidden_size, max_length)\n",
    "        self.attn_out = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.CombinedLayerSS = nn.Linear(self.input_size_stock+self.input_size_sentiment, self.hidden_size)\n",
    "        self.LSTM = nn.LSTM(self.input_size_sentiment+self.input_size_stock, self.hidden_size,num_layers=num_layers, batch_first=True)\n",
    "        self.Linear1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.Linear2 = nn.Linear(self.hidden_size, self.input_size_stock)\n",
    "\n",
    "\n",
    "    def forward(self,x, hidden, stock_news):\n",
    "        attn_wights = F.softmax(self.attn_hidd(hidden[0][0]), dim=1)\n",
    "        context = torch.bmm(attn_wights.unsqueeze(0), stock_news.unsqueeze(0))\n",
    "        x_SS = torch.cat((x,context),dim=2)\n",
    "        out, hidden = self.LSTM(x_SS, hidden)\n",
    "        out = self.Linear1(out)\n",
    "        out = self.Linear2(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 256])\n",
      "torch.Size([10, 10, 1, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "batch2 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [55], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m hidde_stock \u001b[39m=\u001b[39m Stock\u001b[39m.\u001b[39minit_hidden(batch_size_stock)\n\u001b[0;32m     23\u001b[0m \u001b[39m#print (x.shape, hidde_stock[0].shape, out_senti.shape)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m out_stock, hidden_stock \u001b[39m=\u001b[39m Stock(x, hidde_stock, out_senti)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m (out_stock\u001b[39m.\u001b[39mshape) \u001b[39m# [Batch, Seq, Output] -> [1, 1, 5]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Felip\\Miniconda3\\envs\\MLearn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [51], line 24\u001b[0m, in \u001b[0;36mStockPredictor.forward\u001b[1;34m(self, x, hidden, stock_news)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x, hidden, stock_news):\n\u001b[0;32m     23\u001b[0m     attn_wights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_hidd(hidden[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m     context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(attn_wights\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), stock_news\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[0;32m     25\u001b[0m     x_SS \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x,context),dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     26\u001b[0m     out, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLSTM(x_SS, hidden)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: batch2 must be a 3D tensor"
     ]
    }
   ],
   "source": [
    "Senti = SentientAnalizer(1, 128, 256, 10000, 128)\n",
    "Stock = StockPredictor(5,256,256,5,10)\n",
    "# input of sentient\n",
    "batch_size_senti = 10\n",
    "# input of Sentiment\n",
    "# InputSize [Batch, Seq, Input] = [Cantidad de noticias, Cantidad de palabras, Cantidad de palabras]\n",
    "#                                                                               <Recuerda que es 1>\n",
    "seq_len = 5 # padd news to same length\n",
    "noticias = torch.zeros(batch_size_senti,seq_len,1, dtype=torch.long) # example vector\n",
    "hidde_senti = Senti.init_hidden(batch_size_senti)\n",
    "out_senti, hidden_senti = Senti(noticias, hidde_senti)\n",
    "print (out_senti.shape) # [Batch, Seq, Output] -> [10, 5, 256] \n",
    "# Pero solo queremos las ultimas salidas, osea [:,-1,:]\n",
    "out_senti = out_senti[:,-1,:]\n",
    "print (out_senti.shape) # [Batch, Output] -> [10, 256]\n",
    "# input of Stock\n",
    "batch_size_stock = 1\n",
    "# InputSize [Batch, Seq, Input] = [Cantidad de noticias, Cantidad de palabras, Cantidad de variables]\n",
    "contexto_len = 1\n",
    "vars_stock = 6\n",
    "x = torch.zeros(batch_size_stock,contexto_len,vars_stock, dtype=torch.float)\n",
    "hidde_stock = Stock.init_hidden(batch_size_stock)\n",
    "#print (x.shape, hidde_stock[0].shape, out_senti.shape)\n",
    "out_stock, hidden_stock = Stock(x, hidde_stock, out_senti)\n",
    "print (out_stock.shape) # [Batch, Seq, Output] -> [1, 1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593728\n"
     ]
    }
   ],
   "source": [
    "# count the number of parameters\n",
    "num = 0\n",
    "for p in Senti.parameters():\n",
    "    num += p.numel()\n",
    "print (num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset (torch.utils.data.Dataset):\n",
    "    def __init__(self, data_stock, news, target):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.news = news\n",
    "        self.news_labels = news_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index], self.news[index], self.news_labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next 5 days\n",
    "for i in range(5):\n",
    "    out_stock, hidden_stock = Stock(out_stock, hidden_stock, out_senti)\n",
    "    print (out_stock.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71124d1c67a53e10234d0517a0d95499fe806d0feef4c01c9260b743b98b34cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
