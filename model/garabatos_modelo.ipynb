{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING FILE \n",
    "# [ **IGNORE** ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentientAnalizer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, vocab_size, embedding_dim):\n",
    "        super(SentientAnalizer, self).__init__()\n",
    "        \"\"\"\n",
    "        [Vocab_size -> Embedding_size]\n",
    "        input_size: [Seq, Batch, Inputs]\n",
    "        \"\"\"\n",
    "        # input_size == 1\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.Vocab_Embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.LSTM = nn.LSTM(self.embedding_dim, self.hidden_size,num_layers=2, batch_first=True)\n",
    "        self.Linear1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.Linear2 = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self,x, hidden):\n",
    "        emb = self.Vocab_Embedding(x).view(x.shape[0],x.shape[1],-1)\n",
    "        #print (emb.shape)\n",
    "        #print (hidden[0].shape)\n",
    "        out, hidden = self.LSTM(emb, hidden)\n",
    "        out = self.Linear1(out)\n",
    "        out = self.Linear2(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(2, batch_size, self.hidden_size),\n",
    "                torch.zeros(2, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size_stock,input_size_sentiment, hidden_size, num_layers, max_length):\n",
    "        super(StockPredictor, self).__init__()\n",
    "        \"\"\"\n",
    "        [Vocab_size -> Embedding_size]\n",
    "        input_size: [Seq, Batch, Inputs]\n",
    "        \"\"\"\n",
    "        # input_size == 1\n",
    "        self.input_size_stock = input_size_stock\n",
    "        self.input_size_sentiment = input_size_sentiment\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attn_hidd = nn.Linear(self.hidden_size, max_length)\n",
    "        self.attn_out = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.CombinedLayerSS = nn.Linear(self.input_size_stock+self.input_size_sentiment, self.hidden_size)\n",
    "        self.LSTM = nn.LSTM(self.input_size_sentiment+self.input_size_stock, self.hidden_size,num_layers=num_layers, batch_first=True)\n",
    "        self.Linear1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.Linear2 = nn.Linear(self.hidden_size, self.input_size_stock)\n",
    "\n",
    "\n",
    "    def forward(self,x, hidden, stock_news):\n",
    "        attn_wights = F.softmax(self.attn_hidd(hidden[0][0]), dim=1)\n",
    "        context = torch.bmm(attn_wights.unsqueeze(0), stock_news.unsqueeze(0))\n",
    "        x_SS = torch.cat((x,context),dim=2)\n",
    "        out, hidden = self.LSTM(x_SS, hidden)\n",
    "        out = self.Linear1(out)\n",
    "        out = self.Linear2(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "Senti = SentientAnalizer(1, 128, 256, 50000, 128)\n",
    "Stock = StockPredictor(5,256,256,5,10)\n",
    "# input of sentient\n",
    "batch_size_senti = 10\n",
    "# input of Sentiment\n",
    "# InputSize [Batch, Seq, Input] = [Cantidad de noticias, Cantidad de palabras, Cantidad de palabras]\n",
    "#                                                                               <Recuerda que es 1>\n",
    "seq_len = 5\n",
    "noticias = torch.zeros(batch_size_senti,seq_len,1, dtype=torch.long)\n",
    "hidde_senti = Senti.init_hidden(batch_size_senti)\n",
    "out_senti, hidden_senti = Senti(noticias, hidde_senti)\n",
    "print (out_senti.shape) # [Batch, Seq, Output] -> [10, 5, 256] \n",
    "# Pero solo queremos las ultimas salidas, osea [:,-1,:]\n",
    "out_senti = out_senti[:,-1,:]\n",
    "print (out_senti.shape) # [Batch, Output] -> [10, 256]\n",
    "# input of Stock\n",
    "batch_size_stock = 1\n",
    "# InputSize [Batch, Seq, Input] = [Cantidad de noticias, Cantidad de palabras, Cantidad de variables]\n",
    "contexto_len = 1\n",
    "var_stock = 5\n",
    "x = torch.zeros(batch_size_stock,contexto_len,var_stock, dtype=torch.float)\n",
    "hidde_stock = Stock.init_hidden(batch_size_stock)\n",
    "#print (x.shape, hidde_stock[0].shape, out_senti.shape)\n",
    "out_stock, hidden_stock = Stock(x, hidde_stock, out_senti)\n",
    "print (out_stock.shape) # [Batch, Seq, Output] -> [1, 1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset (torch.utils.data.Dataset):\n",
    "    def __init__(self, data_stock, notice, target):\n",
    "        self.data = data_stock\n",
    "        self.notice = notice\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.notice[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 1, 5]) torch.Size([10, 10, 5, 1]) torch.Size([9, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# minidataset\n",
    "data_stock = torch.zeros(10,1,5, dtype=torch.float)\n",
    "notice = torch.zeros(10,10,5,1, dtype=torch.long)\n",
    "target = data_stock[:-1,:,:]\n",
    "data_stock = data_stock[1:,:,:]\n",
    "print (data_stock.shape, notice.shape, target.shape)\n",
    "dataset = Dataset(data_stock, notice, target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[2., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[3., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[4., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[5., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[6., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[7., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[8., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[9., 0., 0., 0., 0.]]])\n",
      "change\n",
      "tensor([[[0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[2., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[3., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[4., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[5., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[6., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[7., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[8., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "test_list_1 = torch.zeros(10,1,5, dtype=torch.float)\n",
    "# fill list with incremental numbers\n",
    "for i in range(10):\n",
    "    test_list_1[i,0,0] = i\n",
    "\n",
    "test_list_2 = test_list_1[:-1,:,:]\n",
    "test_list_1 = test_list_1[1:,:,:]\n",
    "\n",
    "print(test_list_1)\n",
    "print('change')\n",
    "print(test_list_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(list(Senti.parameters())+list(Stock.parameters()), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5]) torch.Size([1, 10, 5, 1]) torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "0.002249381970614195\n",
      "torch.Size([1, 1, 5]) torch.Size([1, 10, 5, 1]) torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "0.000624887878075242\n",
      "torch.Size([1, 1, 5]) torch.Size([1, 10, 5, 1]) torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "2.7504755053087138e-05\n",
      "torch.Size([1, 1, 5]) torch.Size([1, 10, 5, 1]) torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "0.00021058274433016777\n",
      "torch.Size([1, 1, 5]) torch.Size([1, 10, 5, 1]) torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "0.0005327091785147786\n",
      "torch.Size([1, 1, 5]) torch.Size([1, 10, 5, 1]) torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "0.000563718203920871\n",
      "torch.Size([1, 1, 5]) torch.Size([1, 10, 5, 1]) torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "0.0003882246383000165\n",
      "torch.Size([1, 1, 5]) torch.Size([1, 10, 5, 1]) torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "0.00017890959861688316\n",
      "torch.Size([1, 1, 5]) torch.Size([1, 10, 5, 1]) torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "4.147306390223093e-05\n"
     ]
    }
   ],
   "source": [
    "# test the dataloader\n",
    "for i, (data_stock, notice, target) in enumerate(train_loader):\n",
    "    print (data_stock.shape, notice.shape, target.shape)\n",
    "    # Test the model\n",
    "    notice = notice.squeeze(0)\n",
    "    hidde_senti = Senti.init_hidden(notice.shape[0])\n",
    "    out_senti, hidden_senti = Senti(notice, hidde_senti)\n",
    "    out_senti = out_senti[:,-1,:]\n",
    "    hidde_stock = Stock.init_hidden(data_stock.shape[0])\n",
    "    out_stock, hidden_stock = Stock(data_stock, hidde_stock, out_senti)\n",
    "    print (out_stock.shape)\n",
    "    # Test the optimizer\n",
    "    optimizer.zero_grad()   \n",
    "    loss = criterion(out_stock, target)\n",
    "    print (loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n",
      "torch.Size([1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# predict the next 5 days\n",
    "for i in range(5):\n",
    "    out_stock, hidden_stock = Stock(out_stock, hidde_stock, out_senti)\n",
    "    print (out_stock.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv as numpy\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "# news_1 = np.genfromtxt('../data/Preprocess/final_data/Crypto_Currency_News.csv', delimiter=',')\n",
    "news_1 = pd.read_csv('../data/Preprocess/final_data/Crypto_Currency_News.csv')\n",
    "news_2 = pd.read_csv('../data/Preprocess/final_data/CryptoCurrencies.csv')\n",
    "news_3 = pd.read_csv('../data/Preprocess/final_data/CryptoCurrency.csv')\n",
    "news_4 = pd.read_csv('../data/Preprocess/final_data/Cryptomarkets.csv')\n",
    "news_5 = pd.read_csv('../data/Preprocess/final_data/eth.csv')\n",
    "news_6 = pd.read_csv('../data/Preprocess/final_data/ethfinance.csv')\n",
    "news_7 = pd.read_csv('../data/Preprocess/final_data/ethtrader.csv')\n",
    "# stocks_norm = np.genfromtxt('../data/Preprocess/final_data/finance_norm.csv', delimiter=',')\n",
    "stocks_norm = pd.read_csv('../data/Preprocess/final_data/finance_norm.csv')\n",
    "\n",
    "\n",
    "news_dfs = [news_1, news_2, news_3, news_4, news_5, news_6, news_7]\n",
    "\n",
    "# import scaler\n",
    "with open('../data/Preprocess/scaler.pkl', 'rb') as f:\n",
    "    scaler = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change type\n",
    "import ast\n",
    "for df in news_dfs:\n",
    "    for j in range(1,11):        \n",
    "        df['tok_text_'+str(j)] = np.array(df['tok_text_'+str(j)].apply(lambda x: ast.literal_eval (x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1096, 10, 15)\n",
      "(1096, 10, 15)\n",
      "(1096, 10, 15)\n",
      "(1096, 10, 15)\n",
      "(1096, 10, 15)\n",
      "(1096, 10, 15)\n",
      "(1096, 10, 15)\n",
      "(1096, 9)\n",
      "[[    1    60    61    62    63    64    65    66    67    60    63    62\n",
      "     68    69    70]\n",
      " [    1    93    94    60    95    96    60    84    97    98    99   100\n",
      "     82    83   101]\n",
      " [    1   110   111   112   113   114    84    19    56   115    24   116\n",
      "  12421   117   118]\n",
      " [    1   120   121    43   122   123   124   125    67   126     2     0\n",
      "      0     0     0]\n",
      " [    1     5    10   127   129   119    67   130   131   132   133   134\n",
      "    135     2     0]\n",
      " [    1   136   137   138   139   140   141     2     0     0     0     0\n",
      "      0     0     0]\n",
      " [    1   142   143   144   145     2     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    1   147   148   149    67   126   150     2   151     2     0     0\n",
      "      0     0     0]\n",
      " [    1    29   152   153   154     2     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    1    13   155    49    84   156     2     0     0     0     0     0\n",
      "      0     0     0]]\n",
      "[0.0141943663052515 0.0323436818578338 0.0188991089000406\n",
      " 0.054026850394817 0.0657379315893823 0.2413428156795459\n",
      " 0.0253320859530256 0.7960249954340282 0.0347272791544416]\n"
     ]
    }
   ],
   "source": [
    "# convert all dataframes to numpy\n",
    "# news_1_np = news_1.to_numpy()\n",
    "import numpy as np\n",
    "news_np = []\n",
    "for df in news_dfs:\n",
    "    news_np.append(df.to_numpy())\n",
    "\n",
    "stocks_norm_np = stocks_norm.to_numpy()\n",
    "\n",
    "# delete 1st column\n",
    "# news_1_np = news_1_np[:,1:]\n",
    "for i in range(len(news_np)):\n",
    "    news_np[i] = np.delete(news_np[i], 0, 1)\n",
    "stocks_norm_np = stocks_norm_np[:,1:]\n",
    "\n",
    "# reshape from (n,10) lists to (n,10,15) numbers\n",
    "for k in range(len(news_np)):\n",
    "    news_np[k] = np.array([np.array([np.array(news_np[k][i][j]) for j in range(10)]) for i in range(news_np[k].shape[0])])\n",
    "\n",
    "# print\n",
    "for i in range(len(news_np)):\n",
    "    print(news_np[i].shape)\n",
    "print (stocks_norm_np.shape)\n",
    "#first a row\n",
    "print (news_np[0][0])\n",
    "print(stocks_norm_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1095, 9]) torch.Size([1096, 10, 15]) torch.Size([1095, 9])\n"
     ]
    }
   ],
   "source": [
    "# shape dataset to pytorch dataset class    \n",
    "\n",
    "class Dataset (torch.utils.data.Dataset):\n",
    "    def __init__(self, data_stock, notice, target):\n",
    "        self.data = data_stock.unsqueeze(1)\n",
    "        self.notice = notice.unsqueeze(3)\n",
    "        self.target = target.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.notice[idx], self.target[idx]\n",
    "\n",
    "# using news_1 and stocks_norm\n",
    "data_stock = torch.Tensor(stocks_norm_np[:,:]).float()\n",
    "data_news = torch.Tensor(news_np[0]).long()\n",
    "target = data_stock[1:,:]\n",
    "data_stock = data_stock[:-1,:]\n",
    "print (data_stock.shape, data_news.shape, target.shape)\n",
    "\n",
    "# create dataset\n",
    "dataset = Dataset(data_stock, data_news, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0142, 0.0323, 0.0189,  ..., 0.0253, 0.7960, 0.0347],\n",
      "        [0.0149, 0.0155, 0.0058,  ..., 0.0127, 0.7904, 0.0329],\n",
      "        [0.0137, 0.0160, 0.0066,  ..., 0.0118, 0.7981, 0.0232],\n",
      "        ...,\n",
      "        [0.2594, 0.1608, 0.1806,  ..., 0.0757, 0.0952, 0.0987],\n",
      "        [0.2609, 0.1749, 0.1843,  ..., 0.0995, 0.0954, 0.0856],\n",
      "        [0.2606, 0.1095, 0.1615,  ..., 0.0769, 0.1145, 0.1167]])\n",
      "tensor([[0.0149, 0.0155, 0.0058,  ..., 0.0127, 0.7904, 0.0329],\n",
      "        [0.0137, 0.0160, 0.0066,  ..., 0.0118, 0.7981, 0.0232],\n",
      "        [0.0141, 0.0144, 0.0045,  ..., 0.0092, 0.8153, 0.0261],\n",
      "        ...,\n",
      "        [0.2609, 0.1749, 0.1843,  ..., 0.0995, 0.0954, 0.0856],\n",
      "        [0.2606, 0.1095, 0.1615,  ..., 0.0769, 0.1145, 0.1167],\n",
      "        [0.2589, 0.1152, 0.1572,  ..., 0.0749, 0.1119, 0.1429]])\n"
     ]
    }
   ],
   "source": [
    "print(data_stock)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2589, 0.1152, 0.1572, 0.7236, 0.2515, 0.7386, 0.0749, 0.1119, 0.1429]])\n"
     ]
    }
   ],
   "source": [
    "# print last in target\n",
    "print (dataset.__getitem__(len(dataset)-1)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "example of cell in news_1:\n",
    "'[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]'\n",
    "convert to:\n",
    "[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "'''\n",
    "# change from string to list\n",
    "import ast\n",
    "for j in range(1,11):        \n",
    "    news_1['tok_text_'+str(j)] = np.array(news_1['tok_text_'+str(j)].apply(lambda x: ast.literal_eval (x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1096, 10, 15) (1096, 9)\n"
     ]
    }
   ],
   "source": [
    "# convert all dataframes to numpy\n",
    "news_1_np = news_1.to_numpy()\n",
    "stocks_norm_np = stocks_norm.to_numpy()\n",
    "\n",
    "# delete 1st column\n",
    "news_1_np = news_1_np[:,1:]\n",
    "stocks_norm_np = stocks_norm_np[:,1:]\n",
    "\n",
    "# reshape from (n,10) lists to (n,10,15) numbers\n",
    "news_1_np = np.array([np.array([np.array(news_1_np[i][j]) for j in range(10)]) for i in range(news_1_np.shape[0])])\n",
    "\n",
    "# print\n",
    "print (news_1_np.shape, stocks_norm_np.shape)\n",
    "#first a row\n",
    "# print (news_1_np[0])\n",
    "# print(stocks_norm_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1094, 9]) torch.Size([1096, 10, 15]) torch.Size([1094, 9])\n"
     ]
    }
   ],
   "source": [
    "# shape dataset to pytorch dataset class    \n",
    "class Dataset (torch.utils.data.Dataset):\n",
    "    def __init__(self, data_stock, notice, target):\n",
    "        self.data = data_stock.unsqueeze(1)\n",
    "        self.notice = notice.unsqueeze(3)\n",
    "        self.target = target.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.notice[idx], self.target[idx]\n",
    "\n",
    "# using news_1 and stocks_norm\n",
    "# minidataset using the class\n",
    "data_stock = torch.Tensor(stocks_norm_np[:-1,:]).float()\n",
    "data_news = torch.Tensor(news_1_np).long()\n",
    "target = data_stock[1:,:]\n",
    "data_stock = data_stock[:-1,:]\n",
    "print (data_stock.shape, data_news.shape, target.shape)\n",
    "\n",
    "# create dataset\n",
    "dataset = Dataset(data_stock, data_news, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Senti_n = SentientAnalizer(1, 128, 256, 50000, 128)\n",
    "Stock_n = StockPredictor(9,256,256,9,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 9]) torch.Size([1, 10, 15, 1]) torch.Size([1, 1, 9])\n",
      "torch.Size([1, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test the dataloader\n",
    "for i, (data_stock, data_news, target) in enumerate(train_loader):\n",
    "    print (data_stock.shape, data_news.shape, target.shape)\n",
    "    # Test the model\n",
    "    data_news = data_news.squeeze(0)\n",
    "    hidde_senti = Senti_n.init_hidden(data_news.shape[0])\n",
    "    out_senti, hidden_senti = Senti(data_news, hidde_senti)\n",
    "    out_senti = out_senti[:,-1,:]\n",
    "    hidde_stock = Stock_n.init_hidden(data_stock.shape[0])\n",
    "    out_stock, hidden_stock = Stock_n(data_stock, hidde_stock, out_senti)\n",
    "    print (out_stock.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.Adam(list(Senti_n.parameters())+list(Stock_n.parameters()), lr=0.001)\n",
    "# loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0014296459266915917\n",
      "100 0.000650658946169372\n",
      "200 0.0014070377174762844\n",
      "300 0.0011551575679493492\n",
      "400 0.0012379382433368449\n",
      "500 0.002957914513046377\n",
      "600 0.005011761623063346\n",
      "700 0.005140971974256944\n",
      "800 0.004851941173131828\n",
      "900 0.004586146082950529\n",
      "1000 0.004664083888935413\n",
      "0.002988739637658\n",
      "0 0.14213499426841736\n",
      "100 0.008965301318258383\n",
      "200 0.00556265594468415\n",
      "300 0.003933456321178756\n",
      "400 0.0033425372967665914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lcano\\Desktop\\GitHub\\NDG_Project\\model\\modelo.ipynb Celda 20\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lcano/Desktop/GitHub/NDG_Project/model/modelo.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()   \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lcano/Desktop/GitHub/NDG_Project/model/modelo.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out_stock, target)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lcano/Desktop/GitHub/NDG_Project/model/modelo.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lcano/Desktop/GitHub/NDG_Project/model/modelo.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lcano/Desktop/GitHub/NDG_Project/model/modelo.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\lcano\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\lcano\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    hidde_senti = Senti_n.init_hidden(10)\n",
    "    hidde_stock = Stock_n.init_hidden(1)\n",
    "    for i, (data_stock, data_news, target) in enumerate(train_loader):\n",
    "        data_news = data_news.squeeze(0)\n",
    "        out_senti, hidden_senti = Senti(data_news, hidde_senti)\n",
    "        out_senti = out_senti[:,-1,:]\n",
    "        out_stock, hidden_stock = Stock_n(data_stock, hidde_stock, out_senti)\n",
    "        # Test the optimizer\n",
    "        optimizer.zero_grad()   \n",
    "        loss = criterion(out_stock, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print (i, epoch_loss/(i+1))\n",
    "    print (epoch, epoch_loss/(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(Senti_n.state_dict(), 'saved_models/Senti_n.pth')\n",
    "torch.save(Stock_n.state_dict(), 'saved_models/Stock_n.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71124d1c67a53e10234d0517a0d95499fe806d0feef4c01c9260b743b98b34cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
